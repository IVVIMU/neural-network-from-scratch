{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://medium.com/@hunter-j-phillips/overview-the-implemented-transformer-eafd87fe9589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import pad\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchtext\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformer_from_scratch.transformer_instantiate import transformer\n",
    "\n",
    "# importing required libraries\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import spacy\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import portalocker\n",
    "\n",
    "# visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits import mplot3d\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PADDING = 20\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 128\n",
    "D_MODEL = 256\n",
    "D_FFN = 512\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 3\n",
    "N_EPOCHS = 10\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "CLIP = 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: https://huggingface.co/datasets/Helsinki-NLP/opus-100\n",
    "dataset_path = 'datasets/opus-100/en-zh'\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "data_files = {\n",
    "    'train': 'train-00000-of-00001.parquet',\n",
    "    'validation': 'validation-00000-of-00001.parquet',\n",
    "    'test': 'test-00000-of-00001.parquet'\n",
    "}\n",
    "\n",
    "files_exist = all(os.path.exists(os.path.join(dataset_path, file_name)) for file_name in data_files.values())\n",
    "\n",
    "if not files_exist:\n",
    "    en_zh_dataset = load_dataset('Helsinki-NLP/opus-100', 'en-zh')\n",
    "\n",
    "    # save to parquet file in local\n",
    "    for split, file_name in data_files.items():\n",
    "        en_zh_dataset[split].to_parquet(os.path.join(dataset_path, file_name))\n",
    "\n",
    "en_zh_dataset = load_dataset(\n",
    "    \"parquet\", \n",
    "    data_files={\n",
    "        split: os.path.join(dataset_path, file_name) for split, file_name in data_files.items()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sixty-first session</td>\n",
       "      <td>第六十一届会议</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I took some medicine for my mu for my mu my mu...</td>\n",
       "      <td>减轻酸... 酸痛的药 减轻酸痛的药</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a challenge. God is challenging you. He's...</td>\n",
       "      <td>上帝在挑战你，他说你是笨蛋</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh, baby.</td>\n",
       "      <td>.. 寶貝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Lucinda?</td>\n",
       "      <td>- 盧辛達？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Take 'em all.</td>\n",
       "      <td>通通拿下</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I so knowthat. That's what's great.</td>\n",
       "      <td>一点都没错，我很高兴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Harlend thinks you're some kind of criminal ma...</td>\n",
       "      <td>哈兰德 感谢你的罪案策划</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>In short, the US has moved to bring together a...</td>\n",
       "      <td>简言之，美国正在试图团结该地区所有担心中国以邻为壑的贸易和汇率政策的国家。对美国来说，其他八...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>So, why would you want to go into business wit...</td>\n",
       "      <td>你为什么想跟我父亲做生意呢</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   en                                                 zh\n",
       "0                                 Sixty-first session                                            第六十一届会议\n",
       "1   I took some medicine for my mu for my mu my mu...                                 减轻酸... 酸痛的药 减轻酸痛的药\n",
       "2   It's a challenge. God is challenging you. He's...                                      上帝在挑战你，他说你是笨蛋\n",
       "3                                           Oh, baby.                                              .. 寶貝\n",
       "4                                          - Lucinda?                                             - 盧辛達？\n",
       "..                                                ...                                                ...\n",
       "95                                      Take 'em all.                                               通通拿下\n",
       "96                I so knowthat. That's what's great.                                         一点都没错，我很高兴\n",
       "97  Harlend thinks you're some kind of criminal ma...                                       哈兰德 感谢你的罪案策划\n",
       "98  In short, the US has moved to bring together a...  简言之，美国正在试图团结该地区所有担心中国以邻为壑的贸易和汇率政策的国家。对美国来说，其他八...\n",
       "99  So, why would you want to go into business wit...                                      你为什么想跟我父亲做生意呢\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(en_zh_dataset['train']['translation'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    \"\"\"\n",
    "    Load the English tokenizers provided by spaCy\n",
    "\n",
    "    Returns:\n",
    "        spacy_en: English tokenizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    print(\"Loaded English tokenizer.\")\n",
    "    return spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded English tokenizer.\n"
     ]
    }
   ],
   "source": [
    "spacy_en = load_tokenizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, language):\n",
    "    \"\"\"\n",
    "    Split a string into its tokens using the corresponding tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text:     string\n",
    "        language: en or zh    \n",
    "    Returns:\n",
    "        tokenized list of strings \n",
    "    \"\"\"\n",
    "    if language == 'en':\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "    elif language == 'zh':\n",
    "        return list(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yield Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(sentences, language):\n",
    "    \"\"\"\n",
    "    Return the tokens for the appropriate language.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of sentences to tokenize.\n",
    "        language:  The language code ('en' for English, 'zh' for Chinese).\n",
    "\n",
    "    Yields:\n",
    "        sequences based on index\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        yield tokenize(sentence, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(dataset, language, min_freq=2):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from a specified language across all splits (train, test, validation) of a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset:  The dataset object containing the 'train', 'test', and 'validation' splits. Each split should contain a 'translation' field with subfields for each language (e.g., 'en' for English, 'zh' for Chinese).\n",
    "        language: The language code ('en' for English, 'zh' for Chinese) for which the vocabulary will be built. This code should match the subfields in the 'translation' field of the dataset.\n",
    "        min_freq: The minimum frequency a token must have across the dataset to be included in the vocabulary. Tokens appearing fewer times than this threshold will be excluded. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Vocab: A vocabulary object that maps tokens to indices for the specified language. This vocabulary includes special tokens\n",
    "               (\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\") and all tokens that appear at least 'min_freq' times in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    all_sentences = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        all_sentences.extend([item['translation'][language] for item in dataset[split]])\n",
    "    \n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(all_sentences, language),\n",
    "        specials=[\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"],\n",
    "        min_freq=min_freq\n",
    "    )\n",
    "\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(dataset, min_freq=2):\n",
    "    \"\"\"\n",
    "    Load or build the vocabulary for English and Chinese languages.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset objet containing the 'train', 'test', and 'validation' splits\n",
    "        min_freq: minimum frequency needed to include a word in the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        src_vocab: English vocabulary\n",
    "        trg_vocab: Chinese vocabulary  \n",
    "    \"\"\"\n",
    "    vocab_path = \"checkpoints/opus-100-en-zh-vocab.pt\"\n",
    "    os.makedirs(os.path.dirname(vocab_path), exist_ok=True)\n",
    "    if not os.path.exists(vocab_path):\n",
    "        # build the English/Chinese vocabulary if it does not exist\n",
    "        src_vocab = build_vocabulary(dataset, \"en\", min_freq)\n",
    "        trg_vocab = build_vocabulary(dataset, \"zh\", min_freq)\n",
    "        \n",
    "        torch.save((src_vocab, trg_vocab), vocab_path)\n",
    "    else:\n",
    "        # load the vocab if it exists\n",
    "        src_vocab, trg_vocab = torch.load(vocab_path)\n",
    "\n",
    "    print(\"Vocabulary loaded.\\nSource vocab size:\", len(src_vocab), \"\\nTarget vocab size:\", len(trg_vocab))\n",
    "    return src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded.\n",
      "Source vocab size: 81674 \n",
      "Target vocab size: 122394\n"
     ]
    }
   ],
   "source": [
    "src_vocab, trg_vocab = load_vocab(en_zh_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_data, file_name):\n",
    "    \"\"\"\n",
    "    Process raw sentences by tokenizing and converting to integers based on the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        raw_data:  English-Chinese sentence pairs\n",
    "        src_vocab: English vocabulary\n",
    "        trg_vocab: Chinese vocabulary \n",
    "        file_name: Name of the file to save/load processed data\n",
    "    \n",
    "    Returns:\n",
    "        data: tokenized data converted to index based on vocabulary\n",
    "    \"\"\"\n",
    "    processed_data_path = os.path.join('checkpoints', file_name)\n",
    "\n",
    "    if os.path.exists(processed_data_path):\n",
    "        print(f'Loading processed data from {processed_data_path}')\n",
    "        data = torch.load(processed_data_path)\n",
    "    else:\n",
    "        data = []\n",
    "        # loop through each sentence pair\n",
    "        for item in tqdm(raw_data, desc='Processing'):\n",
    "\n",
    "            # tokenize the sentence and convert each word to an integers\n",
    "            src_tensor = torch.tensor([src_vocab[token] for token in tokenize(item['translation']['en'], 'en')], dtype=torch.long) \n",
    "            trg_tensor = torch.tensor([trg_vocab[token] for token in tokenize(item['translation']['zh'], 'zh')], dtype=torch.long)\n",
    "\n",
    "            data.append((src_tensor, trg_tensor))\n",
    "\n",
    "        os.makedirs(os.path.dirname(processed_data_path), exist_ok=True)\n",
    "\n",
    "        print(f'Saving processed data to {processed_data_path}')\n",
    "        torch.save(data, processed_data_path)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_IDX = trg_vocab['<bos>']\n",
    "EOS_IDX = trg_vocab['<eos>']\n",
    "PAD_IDX = trg_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    \"\"\"\n",
    "    Process indexed-sequences by adding <bos>, <eos>, and <pad> tokens.\n",
    "\n",
    "    Args:\n",
    "        data_batch: English-Chinese indexed-sentence pairs\n",
    "\n",
    "    Returns:\n",
    "        two batches: one for English and one for Chinese\n",
    "    \"\"\"\n",
    "    src_batch, trg_batch = [], []\n",
    "\n",
    "    for (src_item, trg_item) in data_batch:\n",
    "        # add <bos> and <eos> indices before and after sentence\n",
    "        src_item = torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0).to(device)\n",
    "        trg_item = torch.cat([torch.tensor([BOS_IDX]), trg_item, torch.tensor([EOS_IDX])], dim=0).to(device)\n",
    "\n",
    "        # add padding\n",
    "        src_batch.append(pad(\n",
    "            src_item,\n",
    "            (0, MAX_PADDING - len(src_item)),\n",
    "            value=PAD_IDX\n",
    "        ))\n",
    "\n",
    "        trg_batch.append(pad(\n",
    "            trg_item,\n",
    "            (0, MAX_PADDING - len(trg_item)),\n",
    "            value=PAD_IDX\n",
    "        ))\n",
    "\n",
    "    return torch.stack(src_batch), torch.stack(trg_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1000000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\IVVIMU\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.588 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Processing: 100%|██████████| 1000000/1000000 [04:34<00:00, 3642.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to checkpoints\\opus-100-en-zh-train-processed.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2000/2000 [00:00<00:00, 3249.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to checkpoints\\opus-100-en-zh-valid-processed.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2000/2000 [00:00<00:00, 3232.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to checkpoints\\opus-100-en-zh-test-processed.pt\n"
     ]
    }
   ],
   "source": [
    "train_data = data_process(en_zh_dataset['train'], 'opus-100-en-zh-train-processed.pt')\n",
    "valid_data = data_process(en_zh_dataset['validation'], 'opus-100-en-zh-valid-processed.pt')\n",
    "test_data = data_process(en_zh_dataset['test'], 'opus-100-en-zh-test-processed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(\n",
    "    to_map_style_dataset(train_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=generate_batch\n",
    ")\n",
    "valid_iter = DataLoader(\n",
    "    to_map_style_dataset(valid_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=generate_batch\n",
    ")\n",
    "test_iter = DataLoader(\n",
    "    to_map_style_dataset(test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=generate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer(\n",
    "    src_vocab=src_vocab,\n",
    "    trg_vocab=trg_vocab,\n",
    "    d_model=D_MODEL,\n",
    "    d_ffn=D_FFN,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    device=device,\n",
    "    dropout=DROPOUT,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 87,650,330 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "    Train the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model:     Transformer model to be trained\n",
    "        iterator:  data to be trained on\n",
    "        optimizer: optimizer for updating parameters\n",
    "        criterion: loss function for updating parameters\n",
    "        clip:      value to help prevent exploding gradients\n",
    "\n",
    "    Returns:\n",
    "        loss for epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(iterator, desc='Training'.ljust(10), leave=True)\n",
    "\n",
    "    # loop through each batch in the iterator\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        # set the source and target batches\n",
    "        src, trg = batch\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # logits for each output\n",
    "        logits = model(src, trg[:, :-1])\n",
    "\n",
    "        # expected output\n",
    "        expected_output = trg[:, 1:]\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(\n",
    "            logits.contiguous().view(-1, logits.shape[-1]),\n",
    "            expected_output.contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the weights\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # update the loss\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model:     Transformer model to be trained\n",
    "        iterator:  data to be evaluated\n",
    "        criterion: loss function for assessing outputs\n",
    "\n",
    "    Returns:\n",
    "        loss for the data\n",
    "    \"\"\"\n",
    "\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # evaluate without updating gradients\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(iterator, desc='Evaluating'.ljust(10), leave=True)\n",
    "        # loop through each batch in the iterator\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "\n",
    "            # set the source and target batches\n",
    "            src, trg = batch\n",
    "\n",
    "            # logits for each output\n",
    "            logits = model(src, trg[:, :-1])\n",
    "\n",
    "            # expected output\n",
    "            expected_output = trg[:, 1:]\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(\n",
    "                logits.contiguous().view(-1, logits.shape[-1]),\n",
    "                expected_output.contiguous().view(-1)\n",
    "            )\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "model_path = 'checkpoints/transformer-translate.pt'\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # save the model when it performs better than the previous run\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "else:\n",
    "    print(\"Model already exists. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, max_seq_len=MAX_SEQ_LEN):\n",
    "    \"\"\"\n",
    "    Translate a English sentence to its Chinese equivalent.\n",
    "\n",
    "    Args:\n",
    "        sentence:    English sentence to be translate to Chinese\n",
    "        model:       Transformer model used for translation\n",
    "        device:      device to perform translation on\n",
    "        max_seq_len: maximum token length for translation\n",
    "\n",
    "    Returns:\n",
    "        src:                    return the tokenized input\n",
    "        trg_input:              return the input to the decoder before the final output\n",
    "        trg_output:             return the final translation, shifted right\n",
    "        attention_probs:        return the attention scores for the decoder heads\n",
    "        masked_attention_probs: return the masked attention scores for the decoder heads\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize and index the provided string\n",
    "    if isinstance(sentence, str):\n",
    "        src = ['<bos>'] + [token.text.lower() for token in spacy_en(sentence)] + ['<eos>']\n",
    "    else:\n",
    "        src = ['<bos>'] + sentence + ['<eos>']\n",
    "\n",
    "    # convert to integers\n",
    "    src_indexes = [src_vocab[token] for token in src]\n",
    "\n",
    "    # convert list to tensor (batch_size, seq_len)\n",
    "    src_tensor = torch.tensor(src_indexes).int().unsqueeze(0).to(device)\n",
    "\n",
    "    # set <bos> token for target generation\n",
    "    trg_indexes = [trg_vocab.get_stoi()['<bos>']]\n",
    "\n",
    "    # generate new tokens\n",
    "    for i in range(max_seq_len):\n",
    "\n",
    "        trg_tensor = torch.tensor(trg_indexes).int().unsqueeze(0).to(device)\n",
    "\n",
    "        # generate the next token\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # generate the logits\n",
    "            logits = model.forward(src_tensor, trg_tensor)\n",
    "\n",
    "            # select the newly predicted token, logits (batch_size, seq_len, vocab_size)\n",
    "            pred_token = logits.argmax(2)[:, -1].item()\n",
    "\n",
    "            if pred_token == trg_vocab.get_stoi()['<eos>'] or i == (max_seq_len - 1):\n",
    "\n",
    "                # decoder input\n",
    "                trg_input = trg_vocab.lookup_tokens(trg_indexes)\n",
    "\n",
    "                # decoder output\n",
    "                trg_output = trg_vocab.lookup_tokens(logits.argmax(2).squeeze(0).tolist())\n",
    "\n",
    "                # return src, trg_input, trg_output, model.decoder.attention_probs, model.decoder.masked_attention_probs\n",
    "                return src, trg_input, trg_output, model.decoder.attention_probs\n",
    "            else:\n",
    "                trg_indexes.append(pred_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentence = 'One of the most famous quotes in the field of artificial intelligence is by Alan Turing: \"Can machines think?\"'\n",
    "\n",
    "# src, trg_input, trg_output, attention_probs, masked_attention_probs = translate(src_sentence, model)\n",
    "src, trg_input, trg_output, attention_probs = translate(src_sentence, model)\n",
    "\n",
    "print(f'source = {src}')\n",
    "print(f'target input = {trg_input}')\n",
    "print(f'target output = {trg_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_visualization(src, trg, attention, n_heads=N_HEADS, n_rows=4, n_cols=2):\n",
    "    \"\"\"\n",
    "    Display the attention matrix for each head of a sequence.\n",
    "\n",
    "    Args:\n",
    "        src: tokenized source sentence\n",
    "        trg: tokenized target sentence\n",
    "        attention: attention scores for the heads\n",
    "        n_heads: number of heads\n",
    "        n_rows: number of rows\n",
    "        n_cols: number of columns\n",
    "    \"\"\"\n",
    "\n",
    "    # ensure the number of rows and columns are equal to the number of heads\n",
    "    assert n_rows * n_cols == n_heads\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 25))\n",
    "\n",
    "    # visualize each head\n",
    "    for i in range(n_heads):\n",
    "\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # select the respective head and make it a numpy array for plotting\n",
    "        _attention = attention.squeeze(0)[i, :, :].cpu().detach().numpy()\n",
    "\n",
    "        # plot the matrix\n",
    "        cax = ax.matshow(_attention, cmap='viridis')\n",
    "\n",
    "        # set the size of the labels\n",
    "        ax.tick_params(labelsize=12)\n",
    "\n",
    "        # set the indices for the tick marks\n",
    "        ax.set_xticks(range(len(src)))\n",
    "        ax.set_yticks(range(len(trg)))\n",
    "\n",
    "        # if the provided sequences are sentences or indices\n",
    "        if isinstance(src[0], str):\n",
    "            ax.set_xticklabels([token.lower() for token in src], rotation=45)\n",
    "            ax.set_yticklabels(trg)\n",
    "\n",
    "        elif isinstance(src[0], int):\n",
    "            ax.set_xticklabels(src)\n",
    "            ax.set_yticklabels(trg)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_visualization(src, trg_input, attention_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilingual evaluation understudy (BLEU) is a commonly used metric to evaluate machine translation models. The score ranges between 0 and 1, with a 1 meaning the prediction and expected translation are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Google’s AutoML documentation, a BLEU score’s value can have the following meanings (in terms of percentage):\n",
    "- < 10: almost useless\n",
    "- 10-19: hard to understand\n",
    "- 20-29: understandable but significant grammatical errors\n",
    "- 30-39: understandable to good\n",
    "- 40-49: high quality\n",
    "- 50-59: high quality, adequate, and fluent\n",
    "- \\> 60: better than human quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, iterator):\n",
    "    \"\"\"\n",
    "    Generate predictions for the provided iterator.\n",
    "\n",
    "    Args:\n",
    "        model: Transformer model to be trained\n",
    "        iterator: data to be evaluated\n",
    "\n",
    "    Returns:\n",
    "        predictions: list of predictions, which are tokenized strings\n",
    "        labels: list of expected output, which are tokenized strings\n",
    "    \"\"\"\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # loop through each batch in the iterator\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "\n",
    "            # predict the output\n",
    "            src, trg_input, trg_output, attention_probs = translate(src_vocab.lookup_tokens(src.tolist()), model)\n",
    "\n",
    "            # prediction | remove <eos> token\n",
    "            predictions.append(trg_output[:-1])\n",
    "\n",
    "            # expected output | add extra dim for calculation\n",
    "            labels.append([trg_vocab.lookup_tokens(trg.tolist())])\n",
    "\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = compute_metrics(model, test_iter)\n",
    "bleu_score(predictions, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
