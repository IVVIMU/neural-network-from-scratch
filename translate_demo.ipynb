{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import pad\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchtext\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformer_from_scratch.transformer_instantiate import transformer\n",
    "\n",
    "# importing required libraries\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import spacy\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import portalocker\n",
    "\n",
    "# visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits import mplot3d\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PADDING = 20\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 128\n",
    "D_MODEL = 256\n",
    "D_FFN = 512\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 3\n",
    "N_EPOCHS = 10\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "CLIP = 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: https://huggingface.co/datasets/Helsinki-NLP/opus-100/tree/main/en-zh\n",
    "# dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-zh\")\n",
    "en_zh_dataset = load_dataset(\"parquet\", data_files={\n",
    "    'train': 'datasets/opus-100/en-zh/train-00000-of-00001.parquet',\n",
    "    'validation': 'datasets/opus-100/en-zh/validation-00000-of-00001.parquet'\n",
    "    'test': 'datasets/opus-100/en-zh/test-00000-of-00001.parquet',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sixty-first session</td>\n",
       "      <td>第六十一届会议</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I took some medicine for my mu for my mu my mu...</td>\n",
       "      <td>减轻酸... 酸痛的药 减轻酸痛的药</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a challenge. God is challenging you. He's...</td>\n",
       "      <td>上帝在挑战你，他说你是笨蛋</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh, baby.</td>\n",
       "      <td>.. 寶貝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Lucinda?</td>\n",
       "      <td>- 盧辛達？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Take 'em all.</td>\n",
       "      <td>通通拿下</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I so knowthat. That's what's great.</td>\n",
       "      <td>一点都没错，我很高兴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Harlend thinks you're some kind of criminal ma...</td>\n",
       "      <td>哈兰德 感谢你的罪案策划</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>In short, the US has moved to bring together a...</td>\n",
       "      <td>简言之，美国正在试图团结该地区所有担心中国以邻为壑的贸易和汇率政策的国家。对美国来说，其他八...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>So, why would you want to go into business wit...</td>\n",
       "      <td>你为什么想跟我父亲做生意呢</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   en  \\\n",
       "0                                 Sixty-first session   \n",
       "1   I took some medicine for my mu for my mu my mu...   \n",
       "2   It's a challenge. God is challenging you. He's...   \n",
       "3                                           Oh, baby.   \n",
       "4                                          - Lucinda?   \n",
       "..                                                ...   \n",
       "95                                      Take 'em all.   \n",
       "96                I so knowthat. That's what's great.   \n",
       "97  Harlend thinks you're some kind of criminal ma...   \n",
       "98  In short, the US has moved to bring together a...   \n",
       "99  So, why would you want to go into business wit...   \n",
       "\n",
       "                                                   zh  \n",
       "0                                             第六十一届会议  \n",
       "1                                  减轻酸... 酸痛的药 减轻酸痛的药  \n",
       "2                                       上帝在挑战你，他说你是笨蛋  \n",
       "3                                               .. 寶貝  \n",
       "4                                              - 盧辛達？  \n",
       "..                                                ...  \n",
       "95                                               通通拿下  \n",
       "96                                         一点都没错，我很高兴  \n",
       "97                                       哈兰德 感谢你的罪案策划  \n",
       "98  简言之，美国正在试图团结该地区所有担心中国以邻为壑的贸易和汇率政策的国家。对美国来说，其他八...  \n",
       "99                                      你为什么想跟我父亲做生意呢  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(en_zh_dataset['train']['translation'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    \"\"\"\n",
    "    Load the English tokenizers provided by spaCy\n",
    "\n",
    "    Returns:\n",
    "        spacy_en: English tokenizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    print(\"Loaded English tokenizer.\")\n",
    "    return spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded English tokenizer.\n"
     ]
    }
   ],
   "source": [
    "spacy_en = load_tokenizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, language):\n",
    "    \"\"\"\n",
    "    Split a string into its tokens using the corresponding tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text:     string\n",
    "        language: en or zh    \n",
    "    Returns:\n",
    "        tokenized list of strings \n",
    "    \"\"\"\n",
    "    if language == 'en':\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "    elif language == 'zh':\n",
    "        return list(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yield Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(sentences, language):\n",
    "    \"\"\"\n",
    "    Return the tokens for the appropriate language.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of sentences to tokenize.\n",
    "        language:  The language code ('en' for English, 'zh' for Chinese).\n",
    "\n",
    "    Yields:\n",
    "        sequences based on index\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        yield tokenize(sentence, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(dataset, language, min_freq=2):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from a specified language across all splits (train, test, validation) of a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset:  The dataset object containing the 'train', 'test', and 'validation' splits. Each split should contain a 'translation' field with subfields for each language (e.g., 'en' for English, 'zh' for Chinese).\n",
    "        language: The language code ('en' for English, 'zh' for Chinese) for which the vocabulary will be built. This code should match the subfields in the 'translation' field of the dataset.\n",
    "        min_freq: The minimum frequency a token must have across the dataset to be included in the vocabulary. Tokens appearing fewer times than this threshold will be excluded. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Vocab: A vocabulary object that maps tokens to indices for the specified language. This vocabulary includes special tokens\n",
    "               (\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\") and all tokens that appear at least 'min_freq' times in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    all_sentences = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        all_sentences.extend([item['translation'][language] for item in dataset[split]])\n",
    "    \n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(all_sentences, language),\n",
    "        specials=[\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"],\n",
    "        min_freq=min_freq\n",
    "    )\n",
    "\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(dataset, min_freq=2):\n",
    "    \"\"\"\n",
    "    Load or build the vocabulary for English and Chinese languages.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset objet containing the 'train', 'test', and 'validation' splits\n",
    "        min_freq: minimum frequency needed to include a word in the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        src_vocab: English vocabulary\n",
    "        trg_vocab: Chinese vocabulary  \n",
    "    \"\"\"\n",
    "    vocab_filename = \"checkpoints/vocab.pt\"\n",
    "    if not os.path.exists(vocab_filename):\n",
    "        # build the English/Chinese vocabulary if it does not exist\n",
    "        src_vocab = build_vocabulary(dataset, \"en\", min_freq)\n",
    "        trg_vocab = build_vocabulary(dataset, \"zh\", min_freq)\n",
    "        \n",
    "        torch.save((src_vocab, trg_vocab), vocab_filename)\n",
    "    else:\n",
    "        # load the vocab if it exists\n",
    "        src_vocab, trg_vocab = torch.load(vocab_filename)\n",
    "\n",
    "    print(\"Vocabulary loaded.\\nSource vocab size:\", len(src_vocab), \"\\nTarget vocab size:\", len(trg_vocab))\n",
    "    return src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded.\n",
      "Source vocab size: 81674 \n",
      "Target vocab size: 122394\n"
     ]
    }
   ],
   "source": [
    "src_vocab, trg_vocab = load_vocab(en_zh_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_data):\n",
    "    \"\"\"\n",
    "    Process raw sentences by tokenizing and converting to integers based on the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        raw_data:  English-Chinese sentence pairs\n",
    "        src_vocab: English vocabulary\n",
    "        trg_vocab: Chinese vocabulary \n",
    "    \n",
    "    Returns:\n",
    "        data: tokenized data converted to index based on vocabulary\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # loop through each sentence pair\n",
    "    for item in tqdm(raw_data, desc='Processing'):\n",
    "\n",
    "        # tokenize the sentence and convert each word to an integers\n",
    "        src_tensor = torch.tensor([src_vocab[token] for token in tokenize(item['translation']['en'], 'en')], dtype=torch.long) \n",
    "        trg_tensor = torch.tensor([trg_vocab[token] for token in tokenize(item['translation']['zh'], 'zh')], dtype=torch.long)\n",
    "\n",
    "        data.append((src_tensor, trg_tensor))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_IDX = trg_vocab['<bos>']\n",
    "EOS_IDX = trg_vocab['<eos>']\n",
    "PAD_IDX = trg_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    \"\"\"\n",
    "    Process indexed-sequences by adding <bos>, <eos>, and <pad> tokens.\n",
    "\n",
    "    Args:\n",
    "        data_batch: English-Chinese indexed-sentence pairs\n",
    "\n",
    "    Returns:\n",
    "        two batches: one for English and one for Chinese\n",
    "    \"\"\"\n",
    "    src_batch, trg_batch = [], []\n",
    "\n",
    "    for (src_item, trg_item) in data_batch:\n",
    "        # add <bos> and <eos> indices before and after sentence\n",
    "        src_item = torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0).to(device)\n",
    "        trg_item = torch.cat([torch.tensor([BOS_IDX]), trg_item, torch.tensor([EOS_IDX])], dim=0).to(device)\n",
    "\n",
    "        # add padding\n",
    "        src_batch.append(pad(\n",
    "            src_item,\n",
    "            (0, MAX_PADDING - len(src_item)),\n",
    "            value=PAD_IDX\n",
    "        ))\n",
    "\n",
    "        trg_batch.append(pad(\n",
    "            trg_item,\n",
    "            (0, MAX_PADDING - len(trg_item)),\n",
    "            value=PAD_IDX\n",
    "        ))\n",
    "\n",
    "    return torch.stack(src_batch), torch.stack(trg_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1000000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\IVVIMU\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Processing: 100%|██████████| 1000000/1000000 [04:47<00:00, 3476.24it/s]\n",
      "Processing: 100%|██████████| 2000/2000 [00:00<00:00, 3101.83it/s]\n",
      "Processing: 100%|██████████| 2000/2000 [00:00<00:00, 3106.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = data_process(en_zh_dataset['train'])\n",
    "val_data = data_process(en_zh_dataset['validation'])\n",
    "test_data = data_process(en_zh_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(\n",
    "    to_map_style_dataset(train_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=generate_batch\n",
    ")\n",
    "val_iter = DataLoader(\n",
    "    to_map_style_dataset(val_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=generate_batch\n",
    ")\n",
    "test_iter = DataLoader(\n",
    "    to_map_style_dataset(test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=generate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer(\n",
    "    src_vocab=src_vocab,\n",
    "    trg_vocab=trg_vocab,\n",
    "    d_model=D_MODEL,\n",
    "    d_ffn=D_FFN,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    device=device,\n",
    "    dropout=DROPOUT,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 87,650,330 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "    Train the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model:     Transformer model to be trained\n",
    "        iterator:  data to be trained on\n",
    "        optimizer: optimizer for updating parameters\n",
    "        criterion: loss function for updating parameters\n",
    "        clip:      value to help prevent exploding gradients\n",
    "\n",
    "    Returns:\n",
    "        loss for epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    # loop through each batch in the iterator\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # set the source and target batches\n",
    "        src, trg = batch\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # logits for each output\n",
    "        logits = model(src, trg[:, :-1])\n",
    "\n",
    "        # expected output\n",
    "        expected_output = trg[:, 1:]\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(\n",
    "            logits.contiguous().view(-1, logits.shape[-1]),\n",
    "            expected_output.contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the weights\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # update the loss\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model:     Transformer model to be trained\n",
    "        iterator:  data to be evaluated\n",
    "        criterion: loss function for assessing outputs\n",
    "\n",
    "    Returns:\n",
    "        loss for the data\n",
    "    \"\"\"\n",
    "\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # evaluate without updating gradients\n",
    "    with torch.no_grad():\n",
    "        # loop through each batch in the iterator\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            # set the source and target batches\n",
    "            src, trg = batch\n",
    "\n",
    "            # logits for each output\n",
    "            logits = model(src, trg[:, :-1])\n",
    "\n",
    "            # expected output\n",
    "            expected_output = trg[:, 1:]\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(\n",
    "                logits.contiguous().view(-1, logits.shape[-1]),\n",
    "                expected_output.contiguous().view(-1)\n",
    "            )\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "model_path = 'checkpoints/transformer.pt'\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # save the model when it performs better than the previous run\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
